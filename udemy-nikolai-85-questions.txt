Question 1
Incorrect
A retail company wants to group customers based on purchasing behavior without having any prior labels or categories for the customers. The company needs to understand hidden patterns and similarities among customers. Which of the following algorithms is the most suitable for this task?

Your answer is incorrect
K-nearest neighbors (KNN)

Explanation
KNN is also supervised, relying on labeled data.

Correct answer
K-means clustering

Explanation
K-means clustering is an unsupervised learning algorithm used to group similar data points (customers in this case) into clusters based on their similarities. This is ideal for situations where there are no predefined labels, and the goal is to uncover hidden patterns.

Linear learner

Explanation
Linear learner is a supervised learning model that predicts outcomes based on a linear relationship.

XGBoost

Explanation
XGBoost is a supervised learning algorithm.

Domain
Data Preparation for Machine Learning
Question 2
Correct
A machine learning specialist is using SageMaker Data Wrangler to prepare a dataset that contains missing values. The specialist must handle these missing values before training the model. Which TWO methods can SageMaker Data Wrangler offer for handling missing data? (Choose two.)

Your selection is correct
Dropping records with missing values

Explanation
One approach is to drop records that have missing values, especially if missing data is minimal.

Scaling the missing values

Explanation
Scaling changes the range of numerical data but doesn’t address missing values.

One-hot encoding missing values

Explanation
One-hot encoding is for converting categorical data to numerical values, not for handling missing values.

Your selection is correct
Imputing missing values using pandas or PySpark

Explanation
Custom transformations in SageMaker Data Wrangler allow using libraries like pandas or PySpark to impute missing values based on the mean, median, or more complex methods.

Dropping the entire dataset

Explanation
Dropping the entire dataset is not a reasonable method for handling missing data.

Domain
Data Preparation for Machine Learning
Question 3
Correct
A healthcare company is deploying a machine learning model using Amazon SageMaker to analyze patient data. Due to the sensitive nature of the data, the company must ensure end-to-end encryption for data at rest and in transit. Additionally, the company wants to audit access to encryption keys used to secure the data.

Which combination of AWS services and features should the company use to meet these requirements? (Choose TWO.)

Enable server-side encryption with Amazon S3 managed keys (SSE-S3) to encrypt data at rest and configure HTTPS for data in transit.

Explanation
While SSE-S3 encrypts data at rest, it does not provide the ability to audit access to encryption keys, which is a key requirement in this scenario.

Your selection is correct
Configure SageMaker to use VPC endpoints to securely transfer data over an encrypted channel.

Explanation
Configuring SageMaker to use VPC endpoints ensures secure communication over encrypted channels, meeting the requirement for data encryption in transit.

Enable Amazon Macie to ensure encryption compliance and automatically detect sensitive data within the storage.

Explanation
Amazon Macie is a data discovery and classification service, not a tool for encryption or managing encryption keys. It helps with compliance but does not directly address encryption requirements.

Use client-side encryption with Amazon S3 SDK to encrypt data before sending it to Amazon S3.

Explanation
Client-side encryption could be an alternative, but it adds operational complexity, and the question does not specify this need.

Your selection is correct
Use AWS Key Management Service (KMS) to manage and audit encryption keys and enable server-side encryption with KMS keys (SSE-KMS).

Explanation
AWS Key Management Service (KMS) provides robust key management capabilities, including auditing access to encryption keys through AWS CloudTrail. SSE-KMS ensures data is encrypted at rest with customer-managed keys.

Domain
Data Preparation for Machine Learning
Question 4
Incorrect
Which of the following is an advantage of using Amazon SageMaker Studio Notebooks over traditional standalone SageMaker notebook instances?

Correct answer
Studio notebooks offer persistent storage for managing multiple notebooks and datasets.

Explanation
SageMaker Studio Notebooks provide persistent storage, allowing users to manage multiple notebooks, store datasets, and access them later. This enables better management of machine learning projects over time.

Standalone notebook instances are cheaper to run than SageMaker Studio notebooks.

Explanation
Cost depends on the instance type and usage, but there is no inherent claim that standalone instances are always cheaper than Studio notebooks.

Your answer is incorrect
Studio notebooks support integration with multiple machine learning libraries, while standalone instances do not.

Explanation
Both Studio and standalone notebook instances support popular machine learning libraries (e.g., TensorFlow, PyTorch), as these are pre-installed in both.

Studio notebooks do not require an IAM role for permissions when accessing AWS resources.

Explanation
Both Studio notebooks and standalone notebook instances require proper IAM roles to ensure that permissions are in place for accessing AWS resources like S3, Lambda, or other services.

Domain
ML Model Development
Question 5
Correct
A law firm wants to automate the extraction of key-value pairs from contracts, identify tabular data, and categorize documents for easier searchability using Amazon Textract. What combination of Textract APIs should they use?

Analyze Document API for key-value pairs and tables, Analyze ID API for categorization

Explanation
The Analyze ID API is specific to identity documents, not contracts.

Analyze Document API for forms, Analyze Lending Workflow API for tables, and Analyze Expenses API for categorization

Explanation
The Analyze Lending Workflow API is specialized for mortgage documents, and the Analyze Expenses API is for financial data, not legal contracts.

Analyze Expenses API for key-value pairs, Analyze ID API for tables, and custom queries for categorization

Explanation
The Analyze Expenses API focuses on invoices and receipts, not contract key-value pairs.

Your answer is correct
Analyze Document API for key-value pairs and tables, and custom queries for document categorization

Explanation
The Analyze Document API extracts key-value pairs and tables, and custom queries can be used to categorize documents based on specific criteria.

Domain
Data Preparation for Machine Learning
Question 6
Correct
A machine learning engineer is designing a workflow for a document processing application. The application uses Amazon Textract to extract text and key-value pairs from scanned documents, but some documents have poor quality, requiring human verification for accuracy. The engineer decides to use Amazon Augmented AI (A2I) to handle low-confidence predictions.
Which workflow design is MOST suitable to integrate Amazon A2I for this task?

Implement a manual process where low-confidence Textract predictions are exported to a CSV file and sent to reviewers via email.

Explanation
A manual process is inefficient and lacks the automated workflow that A2I provides.

Your answer is correct
Directly integrate Amazon A2I with Amazon Textract to automatically route low-confidence predictions to a human review team for validation.

Explanation
Amazon A2I can be directly integrated with Amazon Textract to route low-confidence predictions to human reviewers, simplifying the review process

Use Amazon Rekognition to detect and flag poor-quality scans, and then send them to Amazon A2I for review.

Explanation
While Rekognition can be used for image analysis, it is not relevant for routing text extraction tasks from Textract to A2I.

Use Amazon SageMaker Ground Truth to collect annotations and improve the Textract model accuracy over time.

Explanation
SageMaker Ground Truth is used for creating training datasets and is not designed for real-time human reviews of low-confidence predictions.

Domain
Deployment and Orchestration of ML Workflows
Question 7
Correct
A team of data scientists needs to collaborate on a machine learning project in SageMaker using Jupyter notebooks. They want to easily share their work and continue working from where they left off, with all data stored persistently. Which option should they choose?

Your answer is correct
SageMaker Studio notebooks, as they allow multiple users to collaborate with persistent storage.

Explanation
SageMaker Studio notebooks provide a collaborative environment with persistent storage, allowing multiple users to work on the same project and pick up where they left off.

SageMaker training jobs with custom S3 bucket configurations for sharing the notebook files.

Explanation
SageMaker training jobs do not involve collaborative Jupyter notebooks; they are used for running training jobs, not collaboration on notebooks.

SageMaker notebook instances with IAM roles configured for sharing.

Explanation
SageMaker notebook instances are standalone and not ideal for team collaboration.

AWS Glue with a shared notebook instance for seamless collaboration.

Explanation
AWS Glue is used for ETL jobs and not designed for running Jupyter notebooks or collaborative work.

Domain
ML Model Development
Question 8
Correct
A company is planning to utilize AWS Kinesis Data Firehose for the ingestion of large log files from various devices. Their goal is to ensure data consistency and optimize API call usage. What configuration

Use AWS Glue to handle data ingestion without buffering.

Explanation
AWS Glue is primarily an ETL service and does not inherently provide real-time data ingestion like Kinesis Data Firehose.

Enable immediate data processing in AWS Lambda without buffering.

Explanation
Immediate processing without buffering does not optimize API usage and might not handle data consistency efficiently.

Your answer is correct
Configure Kinesis Data Firehose to buffer data based on maximum time intervals.

Explanation
Configuring Kinesis Data Firehose to buffer data based on time intervals helps in batch processing, which reduces API calls and helps maintain data consistency.

Directly send unbuffered data to Amazon S3 for cost efficiency.

Explanation
Directly sending unbuffered data does not leverage the buffering benefits for optimizing API usage and could lead to inefficiency in handling large data volumes.

Domain
Data Preparation for Machine Learning
Question 9
Incorrect
A machine learning engineer needs to set up a solution for storing and managing files that are frequently accessed initially but then rarely accessed. The engineer wants to minimize storage costs while ensuring files remain available. Which solution is MOST appropriate for this scenario?

Store the files in S3 Standard storage class and manually move them to Glacier Deep Archive after 30 days.

Explanation
S3 Standard is more expensive and suitable for frequently accessed data. Manually moving files to Glacier Deep Archive adds operational overhead.

Correct answer
Store the files in S3 Standard-IA storage class and set up a lifecycle rule to transition them to S3 Glacier after 30 days.

Explanation
S3 Standard-IA (Infrequent Access) is suitable for files that are less frequently accessed but still need to be retrieved quickly when needed. Setting up a lifecycle rule to transition files to S3 Glacier after 30 days further reduces storage costs.

Your answer is incorrect
Store the files in S3 Intelligent-Tiering stora

Explanation
Intelligent-Tiering is designed for data with unpredictable access patterns and might not be the most cost-effective for data known to become infrequently accessed.

Store the files in S3 One Zone-IA storage class and manually delete them after 30 days.

Explanation
S3 One Zone-IA is less durable as it stores data in a single AZ and manually deleting files does not optimize storage costs.

Domain
Data Preparation for Machine Learning
Question 10
Correct
Which of the following scenarios best demonstrates the advantage of using AWS CloudFormation to manage infrastructure?

Your answer is correct
A team needs to ensure the consistent creation and configuration of multiple interconnected resources across different AWS accounts.

Explanation
CloudFormation excels in ensuring consistent and repeatable creation and configuration of resources across different AWS accounts and regions.

An IT department requires ad-hoc creation of resources without predefined configurations.

Explanation
CloudFormation is designed for predefined, template-based creation of resources, not ad-hoc creation without configurations.

A developer manually creates individual resources through the AWS Management Console for a single application.

Explanation
Manually creating resources through the console does not leverage the benefits of CloudFormation, such as consistency and repeatability.

A project manager needs to manually update resource configurations directly on the servers.

Explanation
Manually updating configurations on servers does not utilize CloudFormation’s capabilities of managing infrastructure as code.

Domain
Deployment and Orchestration of ML Workflows
Question 11
Incorrect
A company is using Amazon Kendra to build an AI-powered search engine for their internal document database. They want the search engine to return precise answers based on natural language queries and ensure that the most relevant documents are always prioritized. The company also needs to integrate this search capability into their customer service chatbot built on Amazon Lex. What combination of features should they focus on to achieve this goal?

Your answer is incorrect
Use Kendra’s factoid questions feature and enable Lex’s fallback intent to handle ambiguous queries.

Explanation
Using Kendra’s factoid question feature is useful for specific types of queries, but integrating Lex’s fallback intent will not handle complex queries effectively.

Use Amazon Kendra’s descriptive questions and integrate Lex for keyword-based search optimization.

Explanation
Descriptive questions are useful for detailed queries, but the integration with Lex requires natural language processing rather than just keyword-based search

Leverage Kendra Intelligent Ranking and integrate the bot with AWS Lambda for real-time indexing.

Explanation
While Kendra Intelligent Ranking is correct, AWS Lambda is not necessary for real-time indexing, and Kendra handles indexing automatically.

Correct answer
Integrate Kendra’s index with Lex, enabling natural language queries with Kendra Intelligent Ranking for document re-ranking, and use Lex for handling intents.

Explanation
This option leverages Amazon Kendra’s natural language processing to handle complex queries while re-ranking the results through Kendra Intelligent Ranking. By integrating with Amazon Lex, the chatbot can efficiently handle intents while using Kendra to fetch and prioritize relevant documents.

Domain
Deployment and Orchestration of ML Workflows
Question 12
Correct
A machine learning team is training a deep learning model using Amazon SageMaker and wants to monitor the training process in real time to identify issues like vanishing gradients and overfitting. They also need to automatically track and compare different training experiments, including hyperparameter configurations and model performance, to optimize the model. Which combination of SageMaker features should the team use to achieve these goals?

Use SageMaker Neo to optimize model performance and SageMaker Feature Store to manage training data features

Explanation
SageMaker Neo is for optimizing model deployment, and SageMaker Feature Store manages features, but neither is related to real-time debugging or experiment tracking.

Use SageMaker Studio to manually track training metrics and SageMaker Data Wrangler to handle training data

Explanation
SageMaker Studio offers a UI for development but does not automatically track experiments like SageMaker Experiments, nor does it provide real-time debugging.

Your answer is correct
Use SageMaker Debugger for real-time training monitoring and SageMaker Experiments to track and compare training runs

Explanation
SageMaker Debugger enables real-time monitoring of training issues like vanishing gradients and overfitting, while SageMaker Experiments helps track and compare different training runs, including hyperparameters and performance metrics.

Use SageMaker Ground Truth for data labeling and SageMaker Autopilot for model optimization

Explanation
SageMaker Ground Truth is used for data labeling, and SageMaker Autopilot is for automatic model building, not experiment tracking or real-time monitoring.

Domain
ML Model Development
Question 13
Correct
A healthcare company uses Amazon SageMaker to train machine learning models for patient diagnostics. Due to strict regulatory requirements, they must ensure the security of both their training data and deployed models. They also need to continuously monitor the model's predictions for accuracy and automatically retrain the model if accuracy declines over time. Additionally, they must track access to the model and ensure that sensitive patient data is encrypted. Which combination of AWS services will best meet these security and monitoring requirements? (Choose three.)

AWS Secrets Manager to securely store the model’s hyperparameters

Explanation
While AWS Secrets Manager can securely store sensitive information, it is not typically used for storing model hyperparameters, and hyperparameters can be managed directly within SageMaker

AWS Glue for encrypting data during preprocessing

Explanation
AWS Glue is primarily used for data transformation, not for securing or encrypting data during preprocessing.

Amazon GuardDuty to protect against unauthorized access to the model

Explanation
Amazon GuardDuty is a threat detection service, but monitoring access to the model for security purposes is handled better by CloudTrail in this case.

Your selection is correct
AWS CloudTrail to log access and actions performed on the model

Explanation
AWS CloudTrail logs all access and API actions performed on SageMaker resources, helping track who accessed the model and ensuring auditability.

Your selection is correct
Amazon SageMaker Model Monitor to track the model's prediction accuracy

Explanation
Amazon SageMaker Model Monitor continuously tracks the accuracy of deployed models, and can trigger retraining workflows when model drift or decreased accuracy is detected.

Your selection is correct
AWS Key Management Service (KMS) for encrypting training data and model artifacts

Explanation
AWS KMS can be used to encrypt both the training data stored in Amazon S3 and the model artifacts, ensuring compliance with healthcare data privacy regulations.

Domain
ML Solution Monitoring, Maintenance, and Security
Question 14
Correct
What characteristic best describes a stateless system?

Your answer is correct
Processes each request independently without retaining previous state.

Explanation
A stateless system processes each request independently, without retaining any information about previous interactions.

Maintains context across multiple requests.

Explanation
This is a characteristic of stateful systems.

Stores session data for each user interaction.

Explanation
This describes a stateful system that stores session data.

Requires continuous data stream management.

Explanation
Continuous data stream management can apply to both stateful and stateless systems depending on the context but does not define statelessness specifically.

Domain
ML Model Development
Question 15
Correct
Which of the following is a primary benefit of using Amazon SageMaker Experiments in a machine learning workflow?

Your answer is correct
Capturing and organizing metadata for trials to ensure reproducibility.

Explanation
One of the primary benefits of Amazon SageMaker Experiments is the ability to capture and organize detailed metadata for each trial, ensuring that experiments can be reproduced exactly as they were run, which is critical for collaboration and moving models to production

Offering real-time collaboration for multiple users in a single experiment.

Explanation
Although SageMaker supports collaboration, the core benefit of SageMaker Experiments is tracking metadata and ensuring reproducibility, not real-time collaboration.

Deploying models directly to production environments.

Explanation
SageMaker Experiments is for organizing and tracking experiments, not deploying models. Deployment is handled by other SageMaker features.

Automatically tuning hyperparameters without manual intervention.

Explanation
While SageMaker does offer hyperparameter tuning through other services, SageMaker Experiments specifically focuses on tracking configurations and results, not automatic tuning.

Domain
ML Model Development
Question 16
Correct
What happens when you delete an AWS CloudFormation stack?

Only the stack metadata is deleted, not the resources.

Explanation
Deleting a stack will delete all resources defined within it.

All resources within the stack are retained.

Explanation
Deleting a stack will delete all resources defined within it.

Your answer is correct
All resources defined in the stack are deleted.

Explanation
Deleting a stack will delete all resources defined within it.

The stack is archived for future reference.

Explanation
Deleting a stack will delete all resources defined within it.

Domain
Deployment and Orchestration of ML Workflows
Question 17
Correct
A data scientist wants to monitor and troubleshoot the training of an XGBoost model on Amazon SageMaker to ensure there are no disappearing gradients or overfitting. Which combination of services and steps should the data scientist implement?

Integrate AWS Glue to transform data during training and track overfitting issues

Explanation
AWS Glue is for data processing, not training monitoring or debugging.

Your answer is correct
Enable real-time debugging with SageMaker Debugger and set up built-in rules for gradient vanishing detection

Explanation
SageMaker Debugger provides built-in rules to detect disappearing gradients and overfitting, enabling real-time monitoring of the training process.

Use AWS CloudTrail to monitor model metrics and Amazon Athena to query training logs

Explanation
AWS CloudTrail and Athena are not suitable for real-time model training debugging.

Set up AWS CloudWatch Alarms to detect vanishing gradients and automate hyperparameter tuning

Explanation
AWS CloudWatch Alarms monitor system performance, but Debugger handles training issues like vanishing gradients.

Domain
ML Solution Monitoring, Maintenance, and Security
Question 18
Correct
A company needs to run containerized applications with minimal management of the underlying infrastructure. They also require automatic scaling based on workload. Which ECS launch type should they choose?

EC2 launch type

Explanation
EC2 launch type requires the company to manage the underlying infrastructure themselves.

Your answer is correct
Fargate launch type

Explanation
Fargate launch type is serverless and automatically manages and scales the underlying infrastructure.

External launch type

Explanation
External launch type is for running containers on user-managed on-premises servers or virtual machines.

On-premises launch type

Explanation
This is not a valid ECS launch type.

Domain
Deployment and Orchestration of ML Workflows
Question 19
Correct
A data scientist is training a reinforcement learning model in Amazon SageMaker. They want to experiment with different hyperparameters during training, without manually changing code each time. What approach should the data scientist take to configure the training jobs efficiently?

Manually update the training job parameters in the SageMaker console for each run.

Explanation
Manually updating parameters for each run is tedious and prone to error.

Write a new training script for each set of hyperparameters.

Explanation
Writing a new script for every configuration is inefficient.

Use SageMaker's batch transform functionality for hyperparameter tuning.

Explanation
Batch transform is used for batch inference, not hyperparameter tuning.

Your answer is correct
Use SageMaker's hyperparameter tuning functionality and presets to automate the configuration.

Explanation
SageMaker offers hyperparameter tuning and presets, which allow data scientists to experiment with different configurations efficiently without manually adjusting code or re-running each time.

Domain
ML Model Development
Question 20
Correct
A retail company uses Amazon Comprehend to analyze customer feedback. They want to automatically identify key phrases from product reviews and classify the sentiment of each review. The company also requires deeper analysis of medical-related feedback from a subset of their customers. Which setup is MOST appropriate to meet these requirements?

Use Amazon Comprehend Medical for all reviews to extract key phrases and sentiment analysis.

Explanation
Amazon Comprehend Medical is over-specialized for general feedback and is intended for medical text analysis only.

Deploy a custom model in SageMaker for key phrase extraction and sentiment analysis of all feedback.

Explanation
While SageMaker could be used, Amazon Comprehend already provides pre-trained models for key phrase extraction and sentiment analysis without the need for custom model development.

Use Amazon Textract to extract text from reviews, and Amazon Comprehend Medical for sentiment analysis.

Explanation
Amazon Textract is used for extracting text from documents, but it's not necessary here, as reviews are likely in text form already.

Your answer is correct
Use Amazon Comprehend for general feedback analysis and Amazon Comprehend Medical to process and extract relevant insights from medical feedback.

Explanation
Amazon Comprehend is ideal for general feedback analysis, while Amazon Comprehend Medical is specifically designed to process and extract insights from medical-related data, such as patient records or health-related feedback.

Domain
Deployment and Orchestration of ML Workflows
Question 21
Incorrect
A company wants to ensure consistency between features used in model training and inference. They are using SageMaker Feature Store and need to track the evolution of features over time to reproduce the model's results.
Which feature of SageMaker Feature Store can help them achieve this?



Your answer is incorrect
Lineage Tracking

Explanation
Lineage tracking provides transparency into feature creation but is not focused on versioning

Correct answer
Feature Versioning

Explanation
Feature versioning allows tracking changes to features over time, ensuring that models can be reproduced accurately by maintaining a history of the features.

Offline Store

Explanation
The offline store is for batch processing and storage, not for tracking the evolution of features.

Feature Scaling

Explanation
Feature scaling adjusts the range of values in features but does not help track the evolution of features.

Domain
ML Solution Monitoring, Maintenance, and Security
Question 22
Correct
A machine learning engineer is using Amazon SageMaker Debugger to monitor a TensorFlow model's training. They want to automatically detect overfitting and underfitting. What steps should the engineer take to use SageMaker Debugger effectively? (Choose Two.)

Your selection is correct
Configure debug rules that monitor key metrics such as loss curves during training

Explanation
Defining debug rules is essential for tracking key metrics like loss curves to detect overfitting or underfitting.

Your selection is correct
Set up a debugger hook in the training script to capture relevant tensors

Explanation
Configuring a debugger hook allows the training script to capture and log relevant training metrics.

Use Amazon SageMaker Ground Truth to label additional training data

Explanation
SageMaker Ground Truth is for data labeling, unrelated to model training debugging.

Use AWS Glue to preprocess data before training

Explanation
AWS Glue is for ETL, not for training monitoring.

Manually stop the training job if no improvements are observed

Explanation
SageMaker Debugger automates the detection and alerts without manual intervention.

Domain
Deployment and Orchestration of ML Workflows
Question 23
Correct
A machine learning engineer needs to create an ETL process that automatically extracts data from an Amazon S3 bucket, transforms it, and loads it into Amazon Redshift. Which AWS service should the engineer use to simplify this task?

AWS Lambda

Explanation
AWS Lambda can run code in response to events but is not designed specifically for ETL tasks.

Your answer is correct
AWS Glue

Explanation
AWS Glue is a fully managed ETL service that simplifies the process of extracting data from S3, transforming it, and loading it into Redshift through its visual interface and pre-built transformations.

Amazon Kinesis

Explanation
Amazon Kinesis is designed for real-time data streaming, not for traditional ETL tasks

Amazon EMR

Explanation
Amazon EMR is used for big data processing with frameworks like Hadoop and Spark but requires more manual setup and management compared to AWS Glue.

Domain
Data Preparation for Machine Learning
Question 24
Correct
A data science team is using Amazon SageMaker Studio notebooks to build and train machine learning models. They want to automate the ingestion and transformation of large datasets stored in Amazon S3, and also ensure the trained models can make real-time predictions via an API endpoint. Which two AWS services should the team integrate with SageMaker to achieve this? (Choose Two)

Use AWS Lambda to automate the dataset transformations directly within SageMaker notebooks.

Explanation
While AWS Lambda can handle event-driven tasks, AWS Glue is more suitable for large-scale ETL processes. Lambda is not directly used for transforming large datasets.

Use Amazon EC2 to manage and host the trained model for real-time inference.

Explanation
SageMaker has built-in model deployment capabilities, so using EC2 for hosting models is unnecessary.

Your selection is correct
Use AWS Glue to perform ETL (Extract, Transform, Load) operations on the data before training the model.

Explanation
AWS Glue is an ideal service for automating ETL operations. It can prepare and transform large datasets stored in Amazon S3, making it ready for training in SageMaker.

Your selection is correct
Use Amazon API Gateway to expose the trained model as a REST API endpoint for real-time predictions.

Explanation
Amazon API Gateway can expose a REST API endpoint, allowing real-time predictions from the trained model hosted in SageMaker.

Use Amazon Redshift to store the transformed data and query it before feeding it into the SageMaker model.

Explanation
Although Amazon Redshift can store and query data, SageMaker typically processes data from S3. Glue is more suitable for pre-processing in this scenario.

Domain
Deployment and Orchestration of ML Workflows
Question 25
Incorrect
A team of data scientists is building a recommendation system to suggest movies to users based on their previous viewing behavior and ratings. They are considering using the factorization machines algorithm in Amazon SageMaker. Why is the factorization machines algorithm particularly suited for this task?

It assigns a label to each user and movie, allowing personalized recommendations.

Explanation
Assigning labels is typical in classification tasks, but factorization machines predict preferences rather than assigning labels.

Your answer is incorrect
It creates clusters of similar users and movies based on behavior.

Explanation
Clustering is typically done using algorithms like K-means, not factorization machines.

It identifies outliers in user behavior that may indicate anomalies in preferences.

Explanation
Outlier detection is the focus of algorithms like Random Cut Forest (RCF).

Correct answer
It models interactions between features and is effective for sparse datasets, such as recommendation systems.

Explanation
Factorization machines are ideal for recommendation systems because they model interactions between features (users, movies, ratings) and handle sparse datasets effectively, which is common in such systems where only a few interactions are known.

Domain
ML Model Development
Question 26
Incorrect
A company is training a reinforcement learning model in SageMaker to optimize energy usage in a smart grid. After training the model, they need to evaluate its performance before deployment. Which SageMaker feature is most appropriate for this evaluation?

Your answer is incorrect
Use the SageMaker model monitor to automatically track and evaluate the model's performance on new data.

Explanation
Model Monitor is for monitoring deployed models, not evaluating training performance.

Use Amazon Athena to run SQL queries on the training results to evaluate performance.

Explanation
Athena is used for querying data, not for model evaluation.

Deploy the model to an endpoint and evaluate performance using Amazon QuickSight.

Explanation
QuickSight is used for data visualization, not direct model evaluation.

Correct answer
Use SageMaker's built-in evaluation tools and plot training metrics using SageMaker SDK.

Explanation
SageMaker provides built-in evaluation tools and allows users to plot training metrics using the SageMaker SDK to assess the model’s performance before deployment. This helps users visualize learning progress and adjust as needed.

Domain
ML Model Development
Question 27
Correct
A company is using Amazon Bedrock to build a chatbot for customer support. They want to customize the foundation model using their own data to improve response accuracy and tailor the model to their specific domain. Which two features of Amazon Bedrock will help them achieve this? (Choose Two)

Your selection is correct
Retrieval-augmented generation (RAG) for enhanced knowledge base integration

Explanation
Retrieval-augmented generation (RAG) helps by integrating a knowledge base into the model, allowing for more accurate and context-aware responses based on private data.

Auto-scaling of server instances

Explanation
Auto-scaling benefits server management but does not affect the customization of models.

Using pre-trained models without modification

Explanation
Pre-trained models without modification would not allow for domain-specific customizations, making them less suitable for this requirement.

Provisioned throughput for faster response times

Explanation
Provisioned throughput helps with performance but is unrelated to customizing model behavior with domain-specific data.

Your selection is correct
Fine-tuning models with custom data

Explanation
Fine-tuning allows the company to customize the foundation model with their own data, improving its relevance and accuracy for specific domain-related queries.

Domain
ML Model Development
Question 28
Correct
A machine learning team is using Amazon SageMaker Model Monitor to automatically track both data and model quality in a real-time endpoint. They want to set up monitoring for data drift, bias drift, and feature attribution drift, and they need a solution that can trigger alerts when deviations are detected. How should they set up their monitoring system?

Use SageMaker Ground Truth to label the data in real time and configure SageMaker Debugger to trigger alerts

Explanation
SageMaker Ground Truth is used for labeling, and SageMaker Debugger focuses on training, not monitoring deployed models.

Your answer is correct
Configure SageMaker Model Monitor with prebuilt monitoring schedules and integrate with Amazon CloudWatch to trigger alerts

Explanation
SageMaker Model Monitor allows users to set up prebuilt monitoring schedules for tracking data drift, bias drift, and feature attribution drift. Integration with Amazon CloudWatch enables automatic alerts when deviations occur.

Use AWS Glue for data capture and SageMaker Data Wrangler for drift detection

Explanation
AWS Glue is for ETL, and SageMaker Data Wrangler is for data preprocessing, not model monitoring.

Set up SageMaker Autopilot to automatically monitor model quality and retrain the model

Explanation
SageMaker Autopilot automates model building but does not offer real-time monitoring or alerting features.

Domain
ML Solution Monitoring, Maintenance, and Security
Question 29
Correct
A financial institution is using Amazon Macie to monitor sensitive data within its Amazon S3 buckets. The company needs to ensure compliance with regulations that require regular reporting on personal identifiable information (PII) found in their stored data. Which combination of features in Amazon Macie can the institution use to automate this process and generate regular reports?

Integration with Amazon GuardDuty for PII detection and using custom Lambda functions to scan data.

Explanation
While Amazon GuardDuty detects security threats, it is not designed for PII detection or regular compliance reports.

Your answer is correct
Scheduled sensitive data discovery jobs and Macie’s automated data classification.

Explanation
Amazon Macie supports automated data classification and sensitive data discovery jobs that can be scheduled to run regularly. This allows for ongoing identification of PII and ensures compliance through automated reporting.

Real-time monitoring of bucket access policies and Amazon Inspector reports.

Explanation
Real-time monitoring of bucket policies does not provide insights into the contents of stored data.

Macie’s anomaly detection and manual scanning of buckets.

Explanation
Anomaly detection in Macie focuses on detecting unusual behavior, not directly on PII identification.

Domain
ML Solution Monitoring, Maintenance, and Security
Question 30
Incorrect
A financial services firm needs to monitor transaction data in real-time to detect fraudulent activity as soon as it happens. The solution must handle high-throughput data streams and perform complex event processing to trigger alerts instantly.
What is the MOST effective AWS solution to implement this requirement?

Correct answer
Use Kinesis Data Analytics with SQL applications to analyze and identify potential fraudulent activities immediately.

Explanation
Kinesis Data Analytics allows for the immediate processing of high-throughput streams with SQL or Apache Flink applications, ideal for complex event processing and instant alerting in fraud detection scenarios.

Set up Kinesis Data Firehose to stream data to Amazon S3, using scheduled Amazon Athena queries for detection.

Explanation
This setup introduces a delay between data ingestion and analysis, which is not suitable for real-time fraud detection.

Deploy AWS Lambda functions to manually parse and check each transaction record for fraud indicators.

Explanation
While AWS Lambda offers flexible programming, using it to manually parse high-throughput transactions is inefficient and could lead to processing bottlenecks.

Your answer is incorrect
Implement a Kinesis Data Stream with a standard consumer model to process transactions.

Explanation
The standard consumer model might not provide the necessary speed and complexity of processing required for immediate fraud detection.

Domain
Deployment and Orchestration of ML Workflows
Question 31
Incorrect
A Machine Learning Engineer is running multiple trials to experiment with different algorithms and hyperparameters using Amazon SageMaker Experiments. They want to keep track of the data preprocessing, model training, and evaluation steps separately within each trial. What is the best way to organize and track these steps in SageMaker Experiments?

Correct answer
Use trial components to represent each step of the workflow within a trial.

Explanation
In SageMaker Experiments, trial components are used to represent different stages or steps of a workflow within a trial, such as data preprocessing, model training, and evaluation. This allows the ML engineer to track each step separately and compare results across trials.

Your answer is incorrect
Create multiple trackers for each trial to track different steps.

Explanation
Trackers are used to log metadata, but using multiple trackers to represent steps is not the intended functionality.

Use separate experiments for data preprocessing, model training, and evaluation.

Explanation
Multiple experiments would separate workflows, but they are not necessary to organize individual steps within the same trial.

Store each step as an individual trial within an experiment.

Explanation
Each step should not be a separate trial; a trial represents a full execution of a workflow within an experiment.

Domain
ML Model Development
Question 32
Correct
A team is collaborating on a deep learning project using Amazon SageMaker. They need an environment that allows them to spin up instances with GPUs and share their work easily across multiple users. Which option would be the most suitable for this scenario?

Your answer is correct
SageMaker Studio, where the team can create a shared domain and manage multiple notebooks.

Explanation
SageMaker Studio allows multiple users to collaborate within a shared domain, supports multiple notebooks, and provides integrated GPU support. It is designed for collaborative machine learning workflows.

Amazon EC2 instances running Jupyter notebooks for GPU support and sharing capabilities.

Explanation
While EC2 instances can run Jupyter notebooks and support GPUs, they require manual setup for collaboration and infrastructure management, making them more complex compared to SageMaker Studio.

Standalone SageMaker notebooks that allow each user to have independent instances.

Explanation
Standalone SageMaker notebooks are isolated per user, making them less suitable for shared, collaborative workflows.

SageMaker notebook instances with manually configured IAM roles for each user.

Explanation
While SageMaker notebook instances allow manual IAM role configuration, they do not provide as seamless an environment for collaboration as SageMaker Studio.

Domain
ML Model Development
Question 33
Correct
A financial services company uses Amazon Q Business to generate summaries of meeting transcripts and manage knowledge across the organization. They want to integrate Amazon Q Business with their internal knowledge base and ensure that responses are contextually accurate. Which feature of Amazon Q Business is most suitable for enhancing the accuracy of AI-generated answers by retrieving information from the knowledge base?

Your answer is correct
Retrieval Augmented Generation (RAG)

Explanation
Retrieval Augmented Generation (RAG) enhances the model’s content generation by retrieving information from external sources like a knowledge base. This ensures that the responses are more accurate and aligned with the organization’s context.

Prebuilt Connectors

Explanation
Prebuilt connectors help link data sources but do not directly impact the accuracy of the generated answers.

Amazon Q Business Plugin System

Explanation
Plugins are used for third-party integrations, not for improving the accuracy of responses.

Natural Language Understanding (NLU)

Explanation
NLU is related to understanding text but doesn't enhance answers with external data retrieval.

Domain
ML Model Development
Question 34
Correct
A data science team is building a machine learning pipeline with SageMaker. They need to monitor model performance over time in production to detect potential data drift. Which SageMaker feature is most appropriate for this task?

SageMaker Feature Store

Explanation
Feature Store is for managing and sharing features, not monitoring models.

SageMaker Data Wrangler

Explanation
Data Wrangler is for data preparation, not model monitoring.

SageMaker Experiments

Explanation
SageMaker Experiments is for tracking model experiments during development, not monitoring production models.

Your answer is correct
SageMaker Model Monitor

Explanation
SageMaker Model Monitor allows teams to track the quality of deployed models by monitoring for issues like data drift, helping maintain model performance over time.

Domain
ML Solution Monitoring, Maintenance, and Security
Question 35
Incorrect
A company is using SageMaker Ground Truth to label a large dataset for object detection. To reduce labeling costs, they decide to use a semi-automated approach where simple cases are automatically labeled, and complex cases are sent to human workers.

Which of the following SageMaker Ground Truth features allows this combination of machine learning models and human input?

Your answer is incorrect
Mechanical Turk workforce

Explanation
Mechanical Turk is a workforce option, but it does not describe the active learning process.

Correct answer
Active Learning

Explanation
Active learning in SageMaker Ground Truth uses a combination of machine learning models to automatically label simpler cases, while more complex cases are sent to human workers, helping reduce costs.

3D Point Cloud Labeling

Explanation
3D point cloud labeling is a specific type of labeling, not a process for combining machine learning models and human input.

Private Workforce

Explanation
Private workforce refers to an internal team used for labeling but does not involve automation.

Domain
Data Preparation for Machine Learning
Question 36
Correct
How does AWS Glue's serverless architecture benefit a machine learning engineer when setting up ETL jobs?

Your answer is correct
It automatically scales resources and simplifies infrastructure management.

Explanation
AWS Glue’s serverless architecture automatically scales resources based on the workload, reducing the need for manual infrastructure management.

It requires the engineer to manually manage and scale infrastructure.

Explanation
AWS Glue’s serverless nature means no manual infrastructure management is needed.

It limits the integration to only S3 and Redshift.

Explanation
AWS Glue integrates with various data sources and targets beyond just S3 and Redshift, including databases and other services.

It mandates upfront capacity planning and provisioning.

Explanation
AWS Glue does not require upfront capacity planning; it scales automatically.

Domain
Data Preparation for Machine Learning
Question 37
Correct
A customer support system uses both Amazon Kendra for retrieving relevant support documents and Amazon Lex for handling voice and text interactions. The company wants to ensure that their Kendra index is always up to date with new documents from an S3 bucket, and that Lex handles fallback intents when the user's question is unclear.
What solution ensures that both services are working together optimally?

Implement AWS CloudTrail to monitor the Kendra index and integrate it with Lex’s natural language processing for ambiguous questions.

Explanation
AWS CloudTrail is more suitable for logging and monitoring API calls, not real-time indexing or fallback intent handling.

Use AWS Glue to continuously update the Kendra index and rely on Lex's speech recognition for fallback intent handling.

Explanation
AWS Glue is unnecessary for continuous updates in this case, and relying solely on Lex's speech recognition does not optimize fallback handling.

Your answer is correct
Use Amazon EventBridge to trigger Lambda functions that update the Kendra index and implement Lex's fallback intent for handling ambiguous queries.

Explanation
Amazon EventBridge can trigger Lambda to ensure that new documents are indexed in Kendra automatically. Meanwhile, Lex’s fallback intent ensures that ambiguous user queries are handled appropriately when Kendra doesn’t provide a clear match

Use Amazon S3 Event Notifications to directly update the Kendra index and set up manual intents in Lex for ambiguous queries.

Explanation
S3 Event Notifications can update the Kendra index, but manual fallback intents in Lex are less efficient than using Lex’s built-in fallback intent feature.

Domain
ML Solution Monitoring, Maintenance, and Security
Question 38
Correct
A financial services company is using SageMaker Model Monitor to track the quality of their credit scoring model. They need to evaluate the performance of the model by comparing the predicted outcomes with actual results (ground truth data) that occur after the predictions. Which feature of SageMaker Model Monitor enables this comparison?

Data Drift Detection

Explanation
Data drift detection monitors changes in the input data but doesn’t compare predictions with actual outcomes.

Hyperparameter Tuning for Model Optimization

Explanation
Hyperparameter tuning is related to model optimization, not monitoring model quality.

Feature Attribution Analysis

Explanation
Feature attribution analysis explains feature importance but does not compare predictions to actual results.

Your answer is correct
Ground Truth Ingestion for Model Quality Monitoring

Explanation
SageMaker Model Monitor uses ground truth ingestion to merge the actual outcomes (ground truth) with the model's predictions to evaluate the model’s performance in production.

Domain
Deployment and Orchestration of ML Workflows
Question 39
Correct
What is a primary advantage of a stateful data ingestion system?

Uses fewer computational resources than a stateless system.

Explanation
Stateful systems often require more resources to maintain state information compared to stateless systems.

Processes each new data ingestion request independently.

Explanation
This is characteristic of stateless systems where each request is independent.

Simplifies the ingestion process by not requiring previous context.

Explanation
This describes stateless systems, which do not require previous context.

Your answer is correct
Maintains context of previous data ingestion events to avoid reprocessing the same data.

Explanation
Stateful data ingestion maintains context (e.g., timestamps or offsets) of previous events, which helps avoid reprocessing the same data and ensures efficient data handling.

Domain
Data Preparation for Machine Learning
Question 40
Correct
A company is conducting multiple machine learning experiments using Amazon SageMaker. The team is focused on automating its ML workflows to include data preprocessing, training, hyperparameter tuning, and model deployment. They need to track every step and ensure reproducibility of their models while reducing manual intervention. Which combination of SageMaker features should they use?

SageMaker Studio, SageMaker Clarify, and SageMaker Experiments

Explanation
While SageMaker Studio and Clarify are useful for visualization and bias detection, they do not offer the full automation required for this scenario.

SageMaker Experiments, SageMaker Pipelines, and SageMaker Model Monitor

Explanation
SageMaker Model Monitor is used for monitoring deployed models, which is not the focus here.

SageMaker Experiments, SageMaker Neo, and SageMaker Feature Store

Explanation
SageMaker Neo is for model optimization, and SageMaker Feature Store is for managing features, neither of which directly relate to automating entire ML workflows or tracking experiments.

Your answer is correct
SageMaker Autopilot, SageMaker Experiments, and SageMaker Pipelines

Explanation
SageMaker Autopilot automates the entire ML workflow from data preprocessing to model training and tuning, SageMaker Experiments tracks each experiment, and SageMaker Pipelines orchestrates complex workflows, ensuring automation and reproducibility. This combination minimizes manual intervention while tracking every step.

Domain
Deployment and Orchestration of ML Workflows
Question 41
Correct
A data scientist needs to collaborate on a machine learning project and share notebook access with other team members. They also require the ability to work with pre-installed machine learning libraries such as TensorFlow and PyTorch. Which SageMaker feature best fits this need?

Your answer is correct
SageMaker Studio

Explanation
SageMaker Studio provides an integrated development environment where multiple users can collaborate by sharing notebook access. It comes with pre-installed libraries like TensorFlow and PyTorch, making it easy to start working on machine learning projects without additional setup.

SageMaker Notebook Instances

Explanation
SageMaker Notebook Instances are standalone and do not offer the same level of collaboration and integration as Studio.

SageMaker Model Monitor

Explanation
SageMaker Model Monitor is used for tracking model performance in production, not for notebook collaboration.

SageMaker Ground Truth

Explanation
SageMaker Ground Truth is used for data labeling, not for notebook sharing or machine learning development.

Domain
Deployment and Orchestration of ML Workflows
Question 42
Incorrect
A company has a requirement to process real-time streaming data from IoT devices. The data needs to be ingested continuously and some preliminary analysis should be performed before storing the data in an Amazon S3 bucket. Which combination of services and configurations should be used to achieve this? (Choose two)

Correct selection
Set up the Lambda function to trigger based on events from Amazon Kinesis Data Streams.

Explanation
Configuring a Lambda function to trigger based on Kinesis Data Streams events allows real-time processing of the data.

Your selection is correct
Use Amazon Kinesis Data Streams to collect and stream the data from IoT devices.

Explanation
Amazon Kinesis Data Streams is suitable for collecting and streaming data from IoT devices.

Create an Amazon S3 bucket to directly store raw IoT data without any processing.

Explanation
Storing raw data without preliminary analysis does not meet the requirement of performing preliminary analysis.

Your selection is incorrect
Use AWS Glue to run the analysis on data as it arrives in the S3 bucket.

Explanation
AWS Glue is typically used for ETL processes, not real-time stream processing.

Configure an AWS Lambda function to trigger on S3 'PUT' events to process the streaming data.

Explanation
Lambda should be triggered by Kinesis events, not S3 'PUT' events for streaming data processing.

Domain
Deployment and Orchestration of ML Workflows
Question 43
Correct
A company wants to understand which features most contribute to the predictions of its machine learning model in order to build trust and meet regulatory compliance. Which feature of SageMaker Clarify should the company use, and how does it work?

LIME values for feature contribution analysis

Explanation
LIME is another method for explainability, but SageMaker Clarify specifically uses SHAP values.

Your answer is correct
SHAP values for explainability through feature attribution

Explanation
SageMaker Clarify uses SHAP (Shapley Additive Explanations) values, a game theory-based method, to calculate how individual features impact a model's prediction, providing both global and local explanations. This is crucial for explainability and compliance.

Random Cut Forest algorithm to detect ou

Explanation
Random Cut Forest is used for anomaly detection, not explainability.

K-Nearest Neighbors for calculating feature importance

Explanation
K-Nearest Neighbors is a classification/regression algorithm, not typically used for feature attribution.

Domain
ML Model Development
Question 44
Correct
You are tasked with building an AI application using Amazon Bedrock. The application will experience variable traffic, with periods of very high usage followed by long idle times. Which pricing option is the MOST cost-effective for this use case?

Reserved instance pricing

Explanation
Reserved instance pricing is for long-term, consistent usage but is not relevant in the context of serverless Bedrock, which uses token-based pricing.

Provisioned throughput mode

Explanation
Provisioned throughput is more suitable for large, steady workloads requiring consistent performance, not variable traffic.

Spot instance pricing

Explanation
Spot instance pricing applies to EC2, not Amazon Bedrock, and is irrelevant to token-based pricing models.

Your answer is correct
On-demand pricing

Explanation
On-demand pricing allows you to pay only for what you use, making it ideal for workloads with variable traffic, where usage spikes and drops unpredictably.

Domain
Deployment and Orchestration of ML Workflows
Question 45
Correct
A data science team is using Amazon SageMaker Experiments to manage and compare multiple trials in a complex machine learning workflow. They have several trials, each involving data preprocessing, model training, and model evaluation. The team wants to compare key metrics such as accuracy and loss across all trials to identify the best-performing model. What features of SageMaker Experiments should the team use to accomplish this?

Trial components to directly deploy the best-performing model to production.

Explanation
Trial components are not used for deploying models; they represent the different steps of a workflow such as preprocessing and training. Deployment is handled separately.

Trackers to automatically retune hyperparameters for each trial.

Explanation
Trackers are used for logging metadata but not for retuning hyperparameters.

Your answer is correct
The API and graphical interface for visualizing and comparing performance metrics.

Explanation
SageMaker Experiments offers both an API and a graphical interface in SageMaker Studio, which allows users to visualize and compare key performance metrics, such as accuracy and loss, across different trials to identify the best-performing model.

Trial components to store only the preprocessing steps and discard training data.

Explanation
Trial components track multiple steps in the ML workflow, including preprocessing, training, and evaluation. Discarding training data would defeat the purpose of comparison across trials.

Domain
Deployment and Orchestration of ML Workflows
Question 46
Correct
Which of the following is a method used by SageMaker Data Wrangler to address class imbalance in datasets?

Hyperparameter Optimization

Explanation
Hyperparameter optimization is for tuning the model, not balancing datasets.



Model Tuning

Explanation
Model tuning adjusts model parameters, not class imbalance.

Overfitting

Explanation
Overfitting refers to a model issue, not a method to handle class imbalance.

Your answer is correct
Random Undersampling

Explanation
Random undersampling is one of the techniques used in SageMaker Data Wrangler to handle class imbalance by reducing the number of samples in the majority class.

Domain
Data Preparation for Machine Learning
Question 47
Correct
A data scientist is tasked with predicting house prices based on features like the number of rooms, size of the house, and location. They decide to use a supervised learning model. Which of the following models would be most appropriate for this task?

Your answer is correct
Linear regression

Explanation
Linear regression is a type of supervised learning model used for predicting continuous numerical values, such as house prices. It identifies the relationship between independent features (e.g., rooms, size, location) and the dependent variable (price).

K-means clustering

Explanation
K-means is an unsupervised learning algorithm used for clustering, not for predicting continuous values.

Principal Component Analysis (PCA)

Explanation
PCA is a dimensionality reduction technique used to reduce feature complexity.

Random Cut Forest (RCF)

Explanation
RCF is an anomaly detection algorithm, not used for regression tasks.

Domain
Data Preparation for Machine Learning
Question 48
Correct
A machine learning engineer is setting up a fully managed Jupyter notebook instance in Amazon SageMaker to develop and train models. The engineer wants to use an instance optimized for deep learning and ensure the environment is preconfigured with popular ML libraries. What steps should the engineer follow to achieve this?

Use Amazon EMR to set up a Jupyter notebook environment and install the necessary libraries.

Explanation
Amazon EMR is designed for big data processing and is not the optimal choice for setting up a Jupyter notebook environment for deep learning tasks. It is overkill and not the right tool for machine learning model development, especially when SageMaker provides a more specialized and streamlined environment for this purpose.

Your answer is correct
Spin up an on-demand SageMaker notebook instance, configure GPU settings, and utilize the pre-installed libraries like TensorFlow and PyTorch.

Explanation
Amazon SageMaker provides managed notebook instances that come preconfigured with popular machine learning and deep learning libraries like TensorFlow and PyTorch. By spinning up an on-demand SageMaker notebook instance, the engineer can easily configure GPU settings for deep learning tasks. This is a fully managed solution that requires no manual installation of libraries, making it the most efficient choice for deep learning model development.

Launch an EC2 instance, install Jupyter, and manually install TensorFlow and PyTorch.

Explanation
While launching an EC2 instance and manually installing libraries like TensorFlow and PyTorch could work, it would require much more setup and maintenance compared to using SageMaker. EC2 does not provide the seamless, managed environment that SageMaker does for Jupyter notebooks and machine learning tasks.

Manually configure a server with the required libraries and manage the environment.

Explanation
Manually configuring a server and managing the environment is unnecessary when using Amazon SageMaker. SageMaker offers managed notebook instances that are already optimized for machine learning tasks and come with pre-installed libraries, saving time and effort.

Domain
ML Model Development
Question 49
Correct
A company is implementing SageMaker Pipelines to automate and manage machine learning workflows at scale. The company wants to ensure that the pipeline can handle concurrent workflows efficiently. Which feature of SageMaker Pipelines best supports this requirement?

Your answer is correct
It scales to handle tens of thousands of concurrent machine learning workflows in production.

Explanation
SageMaker Pipelines is designed to handle tens of thousands of concurrent machine learning workflows, making it scalable and efficient for large production environments.

It stores the pipeline definitions as relational database entries for fast retrieval.

Explanation
Pipeline definitions are not stored as relational database entries, but rather as JSON definitions using directed acyclic graphs (DAGs).

It allows the manual execution of each pipeline step for better control.

Explanation
Pipelines automate workflows to reduce manual execution, not increase it.

It integrates with Amazon Athena for real-time querying.

Explanation
Amazon Athena is a query service, unrelated to SageMaker Pipelines' concurrency.

Domain
Deployment and Orchestration of ML Workflows
Question 50
Incorrect
A data scientist is evaluating a binary classification model used for fraud detection. The model correctly identified 150 fraudulent transactions, but it also predicted 50 non-fraudulent transactions as fraud. Which metric will be most useful in assessing the proportion of true fraudulent cases identified by the model?

Precision

Explanation
Precision focuses on how many of the predicted positive cases were correct.

Correct answer
Recall

Explanation
Recall (sensitivity) measures the proportion of actual positive cases (fraudulent transactions) that were correctly identified by the model.

F1 Score

Explanation
F1 score balances precision and recall but does not specifically address recall alone.

Your answer is incorrect
Accuracy

Explanation
Accuracy doesn't distinguish between false positives and false negatives.

Domain
ML Model Development
Question 51
Correct
A company wants to implement an AI-powered assistant using Amazon Q Business to automate tasks and streamline their business operations. The company needs to ensure that sensitive data is protected, and they require fine-grained control over data access and user permissions. How should they configure Amazon Q Business to meet these security requirements?

Configure Amazon Q Business to store all sensitive data in unencrypted form but apply IAM policies for access control.

Explanation
Storing data in unencrypted form poses security risks, even with access control mechanisms.

Configure Amazon Q Business to automatically encrypt all data and rely on predefined user roles for access control.

Explanation
Predefined user roles may not provide the necessary granularity for access cont

Your answer is correct
Integrate Amazon Q Business with AWS Identity and Access Management (IAM) Identity Center to manage user permissions and enable data encryption for sensitive information.

Explanation
Amazon Q Business integrates with IAM Identity Center to manage user identity and permissions, allowing the organization to control who can access specific resources. Additionally, data can be encrypted to ensure security. This ensures compliance with security standards and fine-grained control over access.

Use Amazon Q Business with Amazon S3 to store and protect all generated data, without any need for encryption or access control.

Explanation
Relying solely on Amazon S3 without encryption or access control leaves sensitive data vulnerable.

Domain
ML Solution Monitoring, Maintenance, and Security
Question 52
Correct
A company is using Amazon Rekognition to scan images uploaded to their media library. They want to identify inappropriate content, label objects, and verify users based on face comparisons. Which combination of Amazon Rekognition features is most suitable for these use cases?

Label detection, text detection, and face search

Explanation
Face search is used to find faces across a collection of images, but face comparison is better suited for user verification.

Your answer is correct
Unsafe content detection, label detection, and face comparison

Explanation
Amazon Rekognition provides unsafe content detection for identifying harmful content, label detection for categorizing objects in images, and face comparison for verifying user identity.

Label detection, celebrity recognition, and face liveness detection

Explanation
Celebrity recognition identifies famous individuals, which is not relevant for verifying ordinary users.

Face comparison, text detection, and celebrity recognition

Explanation
Text detection extracts text from images, which is not needed for this scenario, and celebrity recognition is not needed.

Domain
ML Solution Monitoring, Maintenance, and Security
Question 53
Correct
An enterprise wants to deploy Amazon Q Business to assist its customer service team by automating ticket creation in Jira based on customer requests. They also need the assistant to handle tasks like retrieving knowledge from their internal database and managing user permissions. Which of the following features of Amazon Q Business are required to meet these requirements? (Choose two.)

Your selection is correct
Integration with Jira through Amazon Q Business Plugins

Explanation
Amazon Q Business supports third-party integration via plugins, including Jira, to automate tasks such as ticket creation.

Manual setup of underlying infrastructure for third-party tool integrations

Explanation
Amazon Q Business is a managed service, so no manual infrastructure setup is required for integrations.

Use of Amazon Athena for querying data from internal databases

Explanation
Amazon Athena is not directly needed for integration with internal databases in this context.

Deployment of custom APIs for third-party application interaction

Explanation
The service already supports APIs for third-party applications, so custom APIs are not necessary.

Your selection is correct
IAM Identity Center for managing user access and permissions

Explanation
IAM Identity Center provides centralized control over user access and permissions, crucial for managing who can interact with the system.

Domain
Deployment and Orchestration of ML Workflows
Question 54
Correct
A Machine Learning Specialist wants to develop a model using Amazon SageMaker without worrying about infrastructure setup. They prefer an interactive environment for writing code, visualizing data, and training models. Which feature of SageMaker best meets these requirements?

Your answer is correct
Amazon SageMaker Notebooks

Explanation
SageMaker Notebooks are fully managed Jupyter notebooks that provide an interactive environment to write code, visualize data, train models, and deploy them without worrying about setting up or maintaining underlying infrastructure like servers or environments.

Amazon SageMaker Model Monitor

Explanation
Amazon SageMaker Model Monitor is used for monitoring model performance, not for development and training tasks.

Amazon SageMaker Ground Truth

Explanation
Amazon SageMaker Ground Truth is used for labeling data, not for interactive code execution.

Amazon SageMaker Neo

Explanation
Amazon SageMaker Neo is used for optimizing models for edge devices, not for interactive model development.

Domain
ML Model Development
Question 55
Correct
A company needs to automatically resize images uploaded to an Amazon S3 bucket and store the resized images in another S3 bucket. How can this be accomplished using AWS services? (Choose two)

Your selection is correct
Set up an S3 Event Notification to trigger the Lambda function when a new object is created.

Explanation
S3 Event Notifications can be set up to trigger the Lambda function when new objects are created in the bucket.

Use AWS Batch to process the images and store the results in another S3 bucket.

Explanation
AWS Batch is not designed for real-time processing of individual image uploads.

Configure AWS Glue to transform and resize the images as they are uploaded.

Explanation
AWS Glue is typically used for ETL processes, not for image resizing.

Your selection is correct
Create a Lambda function to resize images and configure it to be triggered by S3 'ObjectCreated' events.

Explanation
A Lambda function can be used to resize images and can be triggered by events in the S3 bucket.

Use Amazon EC2 instances to constantly monitor the S3 bucket and resize images.

Explanation
Using EC2 instances for this task would be less efficient and not serverless.

Domain
Deployment and Orchestration of ML Workflows
Question 56
Incorrect
A customer service department wants to use Amazon Transcribe to analyze audio recordings from phone calls. They need to differentiate between multiple speakers, automatically punctuate the transcript, and add custom terms for technical jargon used in their industry. Which features should the company prioritize?

Correct answer
Custom vocabulary, auto punctuation, and speaker identification

Explanation
Custom vocabulary improves transcription accuracy for industry-specific terms, auto punctuation ensures transcripts are readable, and speaker identification distinguishes between speakers.

Your answer is incorrect
Speaker identification, multi-language support, and custom vocabulary

Explanation
Multi-language support is not required for the call recordings if all are in one language.

Batch transcription, speaker identification, and auto punctuation

Explanation
Batch transcription is useful for large volumes, but it's not essential to the specific needs mentioned (technical jargon and speaker identification).

Real-time transcription, speaker identification, and custom vocabulary

Explanation
Real-time transcription is unnecessary for analyzing pre-recorded calls.

Domain
ML Model Development
Question 57
Correct
A media company uses AWS to ingest and process live streaming data from various global events. They need to analyze and respond to the incoming data in real-time. The solution must be capable of scaling automatically to handle varying loads and integrate efficiently with AWS Lambda for real-time processing.
Which configuration would BEST meet these requirements?

Your answer is correct
Use Kinesis Data Streams with a custom AWS Lambda function for data processing.

Explanation
Kinesis Data Streams combined with AWS Lambda allows for real-time data processing and auto-scaling capabilities, fitting the need for immediate analysis and responsiveness.

Deploy Kinesis Data Firehose for direct integration with Amazon Redshift for analytics.

Explanation
Kinesis Data Firehose is more suitable for straightforward loading of data into AWS services without the need for real-time processing.

Implement Managed Apache Flink in Kinesis Data Analytics for complex real-time analytics.

Explanation
While Managed Apache Flink provides robust analytics capabilities, it does not inherently offer the best solution for direct, real-time responsiveness integrated with Lambda.

Set up Kinesis Data Streams with enhanced fan-out and subscribe multiple consumer applications directly.

Explanation
Enhanced fan-out improves consumer throughput but does not address the need for real-time processing and responsiveness provided by AWS Lambda.

Domain
Data Preparation for Machine Learning
Question 58
Correct
A financial services company is using Amazon Fraud Detector to detect fraudulent transactions. They have noticed that during peak traffic times, the model needs to scale quickly to handle the increased number of transactions without losing accuracy. How should they configure their fraud detection system to ensure scalability and real-time detection?

Your answer is correct
Use Amazon Fraud Detector's built-in scalability to automatically adjust based on traffic and maintain fraud detection accuracy.

Explanation
Amazon Fraud Detector is designed to scale automatically based on traffic, maintaining detection accuracy without the need for manual resource management. This ensures that fraud detection remains effective even during peak periods.

Manually provision additional resources in AWS Lambda to scale during peak traffic.

Explanation
Manually provisioning resources introduces unnecessary complexity. Amazon Fraud Detector's scalability is automatic.

Set up an Auto Scaling group with EC2 instances to handle fraud detection requests.

Explanation
While EC2 Auto Scaling can help with general web applications, Amazon Fraud Detector already has built-in scalability and does not require EC2 instances.

Use Amazon SageMaker to train a custom fraud detection model and manually manage its deployment during high-traffic periods.

Explanation
SageMaker is not required here, as Amazon Fraud Detector already provides a scalable, fully managed fraud detection service tailored to real-time fraud detection.

Domain
ML Model Development
Question 59
Correct
A data scientist wants to quickly build and deploy a machine learning model for image classification using Amazon SageMaker. The team has limited expertise in managing infrastructure and wants to focus on solving the business problem. Which solution would be the most efficient for the team to implement?

Manually configure a TensorFlow model and manage it using EC2 instances.

Explanation
Manually configuring a TensorFlow model adds unnecessary complexity.

Your answer is correct
Use SageMaker's built-in image classification algorithm with provided data and hyperparameter tuning.

Explanation
SageMaker’s built-in image classification algorithm is pre-packaged and allows the team to focus on solving the business problem without managing the infrastructure. This is the most efficient solution for their limited expertise.

Use SageMaker Studio to build the model using SageMaker Autopilot.

Explanation
While Autopilot is useful, the question specifies that the team is specifically targeting image classification, which can be easily handled by the built-in algorithm.



Use a custom PyTorch model and deploy using SageMaker Python SDK.

Explanation
Using a custom PyTorch model requires more setup and expertise in infrastructure management.

Domain
ML Model Development
Question 60
Incorrect
A data engineer has created a data transformation flow in SageMaker Data Wrangler that includes handling missing values, feature encoding, and normalization. The team wants to automate this process in future projects and reuse it across multiple datasets. What is the most effective way to automate and share this data preparation workflow?



Manually repeat the steps in SageMaker Studio for each new dataset.

Explanation
Manually repeating the steps is inefficient and does not enable automation or reuse.

Correct answer
Export the flow as Python code and use it in an automated pipeline.

Explanation
SageMaker Data Wrangler allows the export of data transformation flows as Python code, which can then be integrated into automated pipelines for reuse and scalability.

Use Amazon Redshift to apply the transformations directly.

Explanation
While Redshift can handle some transformations, it is not part of the Data Wrangler workflow automation process.

Your answer is incorrect
Save the flow as a Jupyter notebook.

Explanation
While saving as a Jupyter notebook can document the workflow, it is not as efficient for automation and reuse in other projects.

Domain
ML Solution Monitoring, Maintenance, and Security
Question 61
Correct
A company is building a machine learning model to predict loan approvals and wants to ensure that the model is fair and unbiased. They plan to use Amazon SageMaker Clarify for bias detection and explainability. Additionally, the company needs to handle class imbalance in the training data, which has far fewer approved loan applications compared to rejected ones. They also want to automate the end-to-end machine learning pipeline from data preparation to model deployment. Which combination of AWS services should the company use to achieve these goals? (Choose Two.)

Amazon SageMaker Autopilot to detect post-training bias and provide model explainability

Explanation
SageMaker Autopilot is for automated model building, but SageMaker Clarify handles bias detection and explainability

Your selection is correct
Amazon SageMaker Pipelines to automate the ML workflow, including bias detection and model dep

Explanation
Amazon SageMaker Pipelines can automate the entire ML workflow, integrating bias detection with SageMaker Clarify and handling deployment tasks efficiently.

Your selection is correct
Amazon SageMaker Data Wrangler to handle class imbalance by using oversampling techniques

Explanation
SageMaker Data Wrangler is the appropriate tool for addressing class imbalance using techniques like oversampling.

Amazon Athena to analyze the model's explainability reports generated by SageMaker Clarify

Explanation
Amazon Athena is used for querying large datasets in S3, but it’s not designed for analyzing explainability reports.

AWS Glue to detect bias in the dataset before training the model

Explanation
AWS Glue is used for data cataloging and ETL, not for bias detection.

Domain
ML Model Development
Question 62
Correct
A machine learning team wants to quickly build and customize a fraud detection solution using SageMaker Jumpstart. They need to fine-tune the pre-trained model for their specific dataset. Which TWO techniques should they use to tailor the model for their needs? (Choose Two)

Develop a custom algorithm from scratch and integrate it with SageMaker Jumpstart.

Explanation
Developing a custom algorithm from scratch is unnecessary when using SageMaker Jumpstart.

Use prompt engineering to shape the input prompts and encourage the desired response.

Explanation
Prompt engineering is more relevant for tasks like text generation, not directly for fraud detection in this context.

Your selection is correct
Fine-tune the pre-trained model by training it on their custom dataset.

Explanation
Fine-tuning the pre-trained model on custom data makes the model more suitable for specific use cases like fraud detection.

Your selection is correct
Use SageMaker automatic hyperparameter tuning to optimize model performance for their data.

Explanation
Hyperparameter tuning helps optimize the performance of the model on the specific dataset, improving its accuracy.

Manually write a training script to run on SageMaker training jobs.

Explanation
SageMaker Jumpstart already includes pre-trained models, so writing a custom training script is not needed

Domain
ML Model Development
Question 63
Incorrect
A machine learning team has deployed several models using Amazon SageMaker. To ensure the models perform optimally in production, they need to monitor the models for data drift and performance degradation. Additionally, they want to track which version of the dataset and hyperparameters were used in the experiments that produced these models. Which combination of SageMaker services will help them achieve this?

SageMaker Experiments and SageMaker Neo

Explanation
SageMaker Neo is for optimizing models, and while SageMaker Experiments tracks experiments, it does not handle production model monitoring.

Your answer is incorrect
SageMaker Clarify and SageMaker Studio

Explanation
SageMaker Clarify is used for bias detection, and SageMaker Studio is a development environment, neither of which focus on monitoring models in production or tracking experiment details.

SageMaker Feature Store and SageMaker Model Monitor

Explanation
SageMaker Feature Store manages feature data but does not provide model monitoring or experiment tracking.

Correct answer
SageMaker Model Monitor and SageMaker Experiments

Explanation
SageMaker Model Monitor enables the team to continuously monitor deployed models for data drift and performance degradation. SageMaker Experiments tracks which datasets, hyperparameters, and configurations were used in the experiments that produced the models, ensuring traceability and performance evaluation.

Domain
ML Solution Monitoring, Maintenance, and Security
Question 64
Correct
Which feature of Amazon S3 ensures that accidental deletions or overwrites of objects can be recovered?

S3 Lifecycle Policies

Explanation
S3 Lifecycle Policies manage transitions between storage classes, not recovery of deleted or overwritten objects.

S3 Encryption

Explanation
S3 Encryption secures data but does not provide recovery mechanisms for deleted or overwritten objects.

S3 Replication

Explanation
S3 Replication copies objects across buckets in different regions for disaster recovery but does not recover deleted or overwritten objects.

Your answer is correct
S3 Versioning

Explanation
S3 Versioning keeps multiple versions of an object in the same bucket, allowing recovery of previous versions if an object is accidentally deleted or overwritten.

Domain
Data Preparation for Machine Learning
Question 65
Correct
A machine learning engineer is using AWS Glue's interactive sessions to develop ETL scripts. To manage costs effectively, what practice should be followed?

Run interactive sessions continuously to avoid restart costs.

Explanation
Running sessions continuously can lead to higher costs due to prolonged resource usage.

Disable all idle timeouts for continuous operation.

Explanation
Disabling idle timeouts would prevent automatic termination, leading to higher costs.

Your answer is correct
Ensure interactive sessions have a defined idle timeout.

Explanation
Setting a defined idle timeout helps in managing costs by terminating sessions that are not actively used, thus avoiding unnecessary charges.

Always use the maximum number of DPUs for each session.

Explanation
Using the maximum number of DPUs without need increases costs unnecessarily.

Domain
Data Preparation for Machine Learning
Question 66
Correct
A data scientist is using SageMaker Data Wrangler to prepare a dataset for training a machine learning model. The processed data will be stored in Amazon S3 for long-term storage and later used in a training job managed by SageMaker. The team also needs to automate the entire process, ensuring that the data preparation, training, and deployment steps are triggered automatically whenever new data is available in S3.
Which combination of services would best meet this requirement?

Amazon S3, AWS Step Functions, and Amazon Athena

Explanation
While Athena can query data, it is not used for orchestration, and Step Functions with Lambda provides better automation for this scenario.

AWS Glue, Amazon S3, and AWS Batch

Explanation
AWS Glue is for ETL, but Lambda and Step Functions are better suited for automating workflows triggered by new data in S3.

Amazon Redshift, AWS Lambda, and Amazon SageMaker Model Monitor

Explanation
Amazon Redshift is for data warehousing, and Model Monitor is for tracking deployed models, not orchestration.

Your answer is correct
Amazon S3, AWS Lambda, and AWS Step Functions

Explanation
AWS Lambda can be triggered by new objects added to S3, and AWS Step Functions can be used to orchestrate the entire workflow, from data preparation in SageMaker Data Wrangler to training and deployment in SageMaker.

Domain
Deployment and Orchestration of ML Workflows
Question 67
Incorrect
A data science team wants to automate their machine learning workflow using SageMaker Pipelines. After the model is trained and registered, they need to automatically trigger batch inference jobs on new data stored in an Amazon S3 bucket. Which combination of services should they use to achieve this?

Your answer is incorrect
Use Amazon Redshift to process the data and SageMaker Pipelines to deploy the model as an endpoint for inference.

Explanation
While Redshift can process data, deploying the model as an endpoint is unnecessary for batch inference, which is better suited for batch transform jobs.

Use Amazon Athena to query the new data and SageMaker Studio to run batch transform jobs.

Explanation
Amazon Athena is used for querying data, not triggering inference jobs.

Use AWS Lambda to directly transform the data in S3 and register the model in SageMaker Pipelines.

Explanation
AWS Lambda is useful for event-driven tasks but not for batch inference jobs that require SageMaker.

Correct answer
Configure an Amazon CloudWatch Event to trigger a SageMaker batch transform job when new data is uploaded to S3.

Explanation
Amazon CloudWatch Events (or EventBridge) can be configured to detect new data uploads in an Amazon S3 bucket and trigger a SageMaker batch transform job automatically. This integrates well with SageMaker Pipelines, which handles model training and registration.

Domain
Deployment and Orchestration of ML Workflows
Question 68
Correct
A company uses AWS Glue to run ETL jobs that process large datasets stored in Amazon S3. To optimize costs, the machine learning engineer wants to ensure that the ETL jobs are not using more resources than necessary. What steps should the engineer take to achieve this?

Set the jobs to run continuously without idle timeout.

Explanation
Running jobs continuously without an idle timeout can lead to higher costs due to unused compute resources.

Use the default DPU settings without changes.

Explanation
Using the default settings may result in higher costs if the default DPUs are more than what is required for the job.

Your answer is correct
Reduce the number of DPUs to the minimum required.

Explanation
Reducing the number of DPUs to the minimum required helps in optimizing costs by not over-provisioning resources.

Increase the default number of DPUs.

Explanation
Increasing DPUs would increase costs and is unnecessary for optimizing.

Domain
Data Preparation for Machine Learning
Question 69
Correct
A machine learning specialist is using Amazon Bedrock to build a generative AI model. They need to ensure that data used in prompts and model responses are encrypted and stay within the same AWS region. Which two features of Amazon Bedrock help meet these security requirements? (Choose Two.)

Your selection is correct
Data encryption in transit and at rest

Explanation
Amazon Bedrock ensures that data, including model prompts and responses, are encrypted both in transit and at rest, ensuring secure handling of sensitive information.

Integration with Amazon SageMaker

Explanation
While SageMaker enhances models, it does not contribute to ensuring the encryption or regional security of data.

Serverless environment for model deployment

Explanation
The serverless environment simplifies infrastructure management but does not directly impact encryption or regional data policies.

Use of Amazon S3 for large dataset storage

Explanation
S3 storage helps manage large datasets but does not specifically address encryption or regional constraints.

Your selection is correct
AWS Identity and Access Management (IAM)

Explanation
AWS Identity and Access Management (IAM) provides fine-grained control over access to data, ensuring that the right permissions are enforced for security.

Domain
ML Solution Monitoring, Maintenance, and Security
Question 70
Incorrect
A healthcare provider wants to develop a machine learning model that can diagnose skin conditions from images by labeling them as benign or malignant. Which algorithm is best suited for this task in Amazon SageMaker?

Your answer is incorrect
Semantic segmentation

Explanation
Semantic segmentation assigns a label to each pixel in the image, which is more granular than needed here.

Correct answer
Image classification

Explanation
Image classification is the most suitable algorithm when the goal is to assign a single label (benign or malignant) to an entire image based on learned features.

K-means clustering

Explanation
K-means clustering is an unsupervised algorithm for grouping data points, not for classifying images.

Object detection

Explanation
Object detection is used to identify and localize multiple objects in an image, which is not the task at hand.

Domain
ML Model Development
Question 71
Incorrect
A data science team is using SageMaker Jumpstart to deploy a pre-built model for text summarization. They need to further customize the model to ensure the summaries generated are highly relevant to their business needs. Which TWO customization techniques can they use with SageMaker Jumpstart? (Choose Two)

Correct selection
Apply prompt engineering to modify the input prompts and improve the relevance of the text summarization.

Explanation
Prompt engineering can be used to refine how the model responds to input, helping guide the output towards more relevant summaries.

Use SageMaker Model Monitor to automatically adjust the model’s parameters during deployment.

Explanation
Model Monitor is for tracking model performance, not for customizing input or fine-tuning the model.

Your selection is correct
Fine-tune the pre-trained model on their custom dataset to adapt the summarization to their specific domain.

Explanation
Fine-tuning the model on a custom dataset allows the model to become more relevant to the business domain, improving the quality of summaries.

Your selection is incorrect
Use SageMaker Neo to optimize the model for faster inference on edge devices.

Explanation
SageMaker Neo is for optimizing models for edge devices, not for customizing a model’s functionality.

Develop a new custom algorithm using TensorFlow and manually train it on SageMaker.

Explanation
Developing a custom algorithm from scratch is unnecessary when SageMaker Jumpstart provides a pre-built model that can be customized.

Domain
ML Model Development
Question 72
Incorrect
A retail company wants to implement a recommendation engine for their e-commerce platform using Amazon Personalize. They want to recommend products in real-time based on user behavior, such as clicks and past purchases. Additionally, they want to enhance their chatbot built on Amazon Lex by providing personalized product recommendations during the conversation. How can these services be integrated to meet this goal?

Your answer is incorrect
Integrate Amazon Lex with Amazon Kendra's factoid feature and Personalize’s real-time API to generate product recommendations based on search results.

Explanation
While Amazon Kendra handles search queries, it doesn’t integrate directly with Amazon Personalize for real-time recommendations. The focus here is on Lex and Personalize integration.

Use AWS Glue to transform data and integrate Lex with Personalize by directly uploading the product catalog into the Lex bot.

Explanation
AWS Glue is used for ETL, but for real-time personalization, Amazon Personalize’s real-time API should be leveraged, not direct product catalog uploads into Lex.

Use Amazon Kendra to index products and Lex for voice commands, enabling personalized search results based on customer queries.

Explanation
Amazon Kendra is not designed for product recommendation engines; it handles document search queries, not personalized recommendatio

Correct answer
Train an Amazon Personalize model with item interaction data and call the recommendations via an AWS Lambda function within the Amazon Lex bot flow.

Explanation
Amazon Personalize can generate real-time recommendations based on item interaction data, and an AWS Lambda function can be used to call these recommendations and integrate them into the Amazon Lex bot, making the conversation more interactive and personalized.

Domain
ML Model Development
Question 73
Correct
You are tasked with creating a reinforcement learning model in SageMaker to optimize trading strategies for a financial application. What are the key steps you should take to develop and train this model? (Choose Two)

Your selection is correct
Define the trading environment, including the state, actions, and reward function.

Explanation
Defining the environment (state, actions, and reward function) is a critical step in any reinforcement learning problem.

Manually evaluate model performance outside of SageMaker using custom Python scripts.

Explanation
SageMaker provides built-in evaluation tools, so manual evaluation isn't required.

Use AWS Lambda to automatically trigger training when new data is available.

Explanation
Lambda is not necessary for this task; SageMaker handles training directly.

Your selection is correct
Use the SageMaker RL estimator and train the model using local mode for faster prototyping.

Explanation
The SageMaker RL estimator helps you easily train models in local mode, allowing for quick iteration during development.

Deploy the model to an Amazon RDS instance to scale performance.

Explanation
SageMaker endpoints are used for deployment, not RDS.

Domain
Deployment and Orchestration of ML Workflows
Question 74
Correct
In SageMaker Pipelines, how are the relationships between workflow steps defined?

By setting up event-driven notifications in Amazon SNS.

Explanation
Amazon SNS is not directly used to define step relationships in SageMaker Pipelines.

Your answer is correct
Through a directed acyclic graph (DAG) that specifies dependencies and order.

Explanation
The relationships between steps in SageMaker Pipelines are defined using a directed acyclic graph (DAG). This structure outlines the dependencies and sequence of each step in the pipeline.

Using a sequence of manual executions managed by the user.

Explanation
Manual executions are not required as the pipeline automates workflows.

With an AWS Lambda function that triggers each step based on conditions.

Explanation
While Lambda could be used for triggers, the main structure in SageMaker Pipelines relies on DAGs.

Domain
ML Model Development
Question 75
Correct
What is the primary difference between AWS Shield Standard and AWS Shield Advanced?

Shield Standard provides protection against all types of DDoS attacks, while Shield Advanced does not.

Explanation
Shield Standard provides protection against the most common DDoS attacks, but Shield Advanced offers additional protections and features.

Shield Advanced only protects Amazon S3 buckets.

Explanation
Shield Advanced protects a range of AWS resources, not just Amazon S3 buckets.

Your answer is correct
Shield Advanced offers enhanced protection and additional features such as detailed attack diagnostics and cost protection.

Explanation
Shield Advanced offers enhanced protection, detailed attack diagnostics, and cost protection against scaling charges due to DDoS attacks.

Shield Standard is a paid service, while Shield Advanced is free.

Explanation
Shield Standard is automatically enabled and free, while Shield Advanced is a paid service.

Domain
ML Solution Monitoring, Maintenance, and Security
Question 76
Correct
A company has deployed a real-time endpoint using Amazon SageMaker to predict product demand. They want to continuously monitor the model’s performance in production, specifically looking for changes in data quality and prediction accuracy. The team wants to receive automated alerts if there is a drift in the incoming data or a decline in model accuracy. Which combination of SageMaker features and services should the company use to achieve this? (Choose Two.)

Your selection is correct
SageMaker Model Monitor for detecting data and model quality drift

Explanation
SageMaker Model Monitor is designed to detect data drift, model quality drift, and other performance issues in production.

AWS Glue for scheduling model monitoring jobs

Explanation
AWS Glue is used for ETL operations, not model monitoring or scheduling.

Your selection is correct
Amazon CloudWatch for setting up alerts on drift and performance metrics

Explanation
Amazon CloudWatch allows users to set up alerts for deviations in model performance or data drift based on metrics captured by SageMaker Model Monitor.

SageMaker Ground Truth for tracking model prediction accuracy

Explanation
SageMaker Ground Truth is used for data labeling, not model performance tracking.

SageMaker Neo for optimizing model inference speed

Explanation
SageMaker Neo optimizes inference speed but is not used for model monitoring.

Domain
ML Solution Monitoring, Maintenance, and Security
Question 77
Correct
A data scientist wants to use one of Amazon SageMaker's built-in algorithms to solve a regression problem. They are considering the SageMaker Linear Learner algorithm because they need a straightforward, easy-to-deploy solution without managing custom infrastructure or creating their own containers. Which of the following steps are required to use SageMaker’s built-in algorithms for this task? (Choose two.)

Manually configure the infrastructure for the training environment.

Explanation
The infrastructure is automatically managed by SageMaker when using built-in algorithms, eliminating the need for manual configuration.

Use a custom machine learning framework like PyTorch or TensorFlow to modify the algorithm.

Explanation
There is no need to modify the algorithm using custom frameworks like PyTorch or TensorFlow; SageMaker provides ready-to-use solutions.

Your selection is correct
Bring in the data and supply it to the SageMaker built-in algorithm.

Explanation
To use SageMaker's built-in algorithms, the data scientist needs to bring their data and supply it to the algorithm, which will handle the training.

Set up and manage custom containers with the algorithm and hyperparameters.

Explanation
Built-in algorithms do not require setting up custom containers; they come pre-packaged with managed infrastructure.

Your selection is correct
Select and define the hyperparameters for the algorithm.

Explanation
The user also needs to define hyperparameters for the algorithm to train effectively. SageMaker can also perform automatic hyperparameter tuning.

Domain
ML Model Development
Question 78
Correct
What is the primary purpose of the AWS Well-Architected Framework Tool?

Your answer is correct
To provide best practices and guidance for building secure, high-performing, resilient, and efficient infrastructure

Explanation
This choice is correct because the primary purpose of the AWS Well-Architected Framework Tool is to provide best practices and guidance for building secure, high-performing, resilient, and efficient infrastructure. It helps organizations design and implement reliable architectures that align with AWS best practices.
To manage billing and cost optimization for AWS services.

Explanation
Managing billing and cost optimization for AWS services is not the primary purpose of the AWS Well-Architected Framework Tool. While cost optimization is an important aspect of cloud architecture, this tool specifically focuses on architectural best practices rather than financial management.
To automate the deployment of AWS resources.

Explanation
The AWS Well-Architected Framework Tool is not designed to automate the deployment of AWS resources. Its primary focus is on providing best practices and guidance for building secure, high-performing, resilient, and efficient infrastructure.
To monitor real-time metrics and performance of AWS applications.

Explanation
Monitoring real-time metrics and performance of AWS applications is not the primary purpose of the AWS Well-Architected Framework Tool. While monitoring and performance optimization are important considerations in cloud architecture, this tool is specifically designed to provide guidance on building secure, high-performing, resilient, and efficient infrastructure.
Domain
Deployment and Orchestration of ML Workflows
Question 79
Correct
A machine learning specialist is tasked with preparing data for model training using SageMaker Data Wrangler. The dataset contains categorical features such as "Product Type" with multiple categories. The chosen machine learning algorithm can only handle numerical inputs. Which technique offered by Data Wrangler would be the best fit to handle this requirement?

Recursive feature elimination

Explanation
Recursive feature elimination is a technique for feature selection, not for converting categorical data.

Imputation

Explanation
Imputation is used to handle missing values, not to convert categorical data into numerical features.

Your answer is correct
One-hot encoding

Explanation
One-hot encoding is the process of converting categorical data into numerical features by creating binary columns for each category. This is a common technique to handle categorical features in machine learning models that require numerical input.

Data normalization

Explanation
Data normalization is used to scale features but does not apply to categorical data conversion.

Domain
Data Preparation for Machine Learning
Question 80
Incorrect
A Machine Learning Specialist is using Amazon SageMaker Clarify to detect potential biases in their model. They want to ensure both the input data and model are fair. What steps can SageMaker Clarify take to assist in this? (Choose Two.)

Your selection is incorrect
Automatically optimize model hyperparameters to reduce bias

Explanation
SageMaker Clarify focuses on bias detection and explainability, not model optimization.

Your selection is correct
Provide post-training bias analysis for model predictions

Explanation
Post-training bias detection helps analyze biases that may be present in the model predictions after training.

Replace unbalanced classes with synthetic data points

Explanation
While Data Wrangler can generate synthetic data, this is not a direct function of SageMaker Clarify.

Automatically remove biased features from the dataset

Explanation
Clarify generates reports with potential biases but does not automatically remove features.

Correct selection
Detect pre-training bias in the dataset

Explanation
SageMaker Clarify offers pre-training bias detection, allowing for the identification of biases in input data before model training.

Domain
ML Model Development
Question 81
Correct
A company wants to set up an event-driven architecture using AWS Lambda. They need to automatically process files as soon as they are uploaded to an Amazon S3 bucket. Which steps should be taken to achieve this setup? (Choose two)

Your selection is correct
Create a Lambda function and configure an S3 bucket to trigger the function on 'PUT' events.

Explanation
Creating a Lambda function and configuring the S3 bucket to trigger the function on 'PUT' events automates the processing of files as they are uploaded.

Your selection is correct
Attach a role to the Lambda function that has permissions to access both the source and destination S3 buckets.

Explanation
Attaching a role with the necessary permissions ensures the Lambda function can access both the source and destination buckets.

Configure the Lambda function to be triggered by Amazon CloudWatch Events.

Explanation
CloudWatch Events are not necessary for S3-triggered Lambda functions.

Manually start the Lambda function each time a file is uploaded.

Explanation
The function should be triggered automatically, not manually.

Set up Amazon SNS to notify the Lambda function when a file is uploaded.

Explanation
While SNS can notify Lambda, it's not required for this scenario since S3 notifications are directly configured.

Domain
Deployment and Orchestration of ML Workflows
Question 82
Correct
A robotics company is using Amazon SageMaker to train reinforcement learning models that control the movement of robots in complex environments. After training the model, they want to deploy it for continuous use in real-world scenarios. What is the best approach for deployment and monitoring of this model?

Run the trained model on EC2 instances and use CloudWatch for monitoring.

Explanation
SageMaker's managed services are more suited to ML model deployment than using EC2 directly.

Use AWS Lambda to periodically deploy and monitor the model.

Explanation
AWS Lambda is not designed for continuous deployment of reinforcement learning models.

Use Amazon S3 to deploy the model and monitor it using AWS Trusted Advisor.

Explanation
S3 is used for storage, not model

Your answer is correct
Deploy the model using SageMaker's managed endpoints and monitor performance with Amazon CloudWatch.

Explanation
SageMaker's managed endpoints allow for scalable deployment of machine learning models, and performance can be monitored using Amazon CloudWatch.

Domain
ML Solution Monitoring, Maintenance, and Security
Question 83
Correct
A logistics company is using Amazon SageMaker to train a reinforcement learning model for optimizing warehouse operations. They want to define the environment where the model will interact with the current state, take actions, and receive rewards. What is the primary consideration when defining the state in a reinforcement learning environment?

The state should include the entire history of the environment's actions and outcomes.

Explanation
The state doesn’t need to include the entire history—just the present is necessary.

Your answer is correct
The state should represent the current situation of the environment relevant to decision-making.

Explanation
In reinforcement learning, the state represents the current situation or configuration of the environment relevant to the agent's decision-making. It doesn't require the entire history of actions, but should provide enough information for the agent to act effectively.

The state should only contain the actions the agent can take at any given time.

Explanation
The actions available are separate from the state, which represents the environment's situation.

The state should be a combination of future predictions and current conditions.

Explanation
Future predictions are not part of the state; it focuses on the present.

Domain
Data Preparation for Machine Learning
Question 84
Correct
A machine learning team is designing a SageMaker Pipeline to train, register, and deploy models. The pipeline uses several parameters for customization. Which of the following statements about parameters in SageMaker Pipelines is correct?

Parameters must always be defined by the user before pipeline execution.

Explanation
Parameters can have default values, so user input is not always required.

Parameters are used exclusively for defining step relationships in the pipeline.

Explanation
Parameters are not exclusively used for step relationships; they customize various aspects of pipeline execution.

Parameters only support string and integer types.

Explanation
Parameters in SageMaker Pipelines support various types, including string, integer, float, and boolean.

Your answer is correct
Parameters can have default values but can also be overridden during execution.

Explanation
Parameters in SageMaker Pipelines can have default values, but they can be overridden with specific values during pipeline execution

Domain
Deployment and Orchestration of ML Workflows
Question 85
Correct
A retail company has deployed a demand forecasting model using Amazon SageMaker. They want to ensure the model remains accurate over time by monitoring both data quality and model performance. They also need to automate the retraining of the model if significant data drift or performance degradation is detected.
Which combination of AWS services and features would best help the company achieve this?

Your answer is correct
Use SageMaker Model Monitor for detecting data drift, Amazon CloudWatch for setting alerts, and SageMaker Pipelines for automating model retraining

Explanation
SageMaker Model Monitor detects data and model drift, while Amazon CloudWatch can trigger alerts based on these metrics. SageMaker Pipelines can automate the retraining workflow when performance degradation is detected.

Use SageMaker Data Wrangler for detecting data drift, AWS Glue for data processing, and SageMaker Debugger for retraining the model

Explanation
SageMaker Data Wrangler is used for data preprocessing, not for detecting drift or automating retraining.

Use SageMaker Ground Truth for monitoring data drift, Amazon CloudWatch for logging, and AWS Step Functions for model deployment

Explanation
SageMaker Ground Truth is for labeling data, not monitoring models, and AWS Step Functions manage workflows but are not specific to model deployment.

Use SageMaker Neo for monitoring model performance, AWS Lambda for data drift detection, and Amazon Kinesis for real-time predictions

Explanation
SageMaker Neo optimizes model performance for inference but doesn't monitor drift, and AWS Lambda is not used for data drift detection.

Domain
Deployment and Orchestration of ML Workflows
